---
type: docs
bookToc: True
weight: 1
---
# **Accelerating Transformers via Conditional Computation: As Aspect of Mixture of Depths**
*Posted by: Inkwan Hwang, Minjae Park*

---

<p align="center">
    <img src=./Pondering_Transformer.jpg> 
</p>
This image was generated by DALL·E 3.

## **Introduction**
“Choice and concentration” is an effective strategies for achieving success in problems. Sometimes, it is not necessary to consume same amount of effort and time into all problems. Expending energy on trivial issues may fail to concentrate on what truly matters. Similarly, in language models, there is a technique that does not focus equally on all tokens but allocates less budget to non-essential tokens. This technique is called conditional computation.

In this post, We will explain conditional computation strategies for Transformers, focusing on a technology announced this year called **Mixture-of-Texture.**

paper: <U><a href="https://arxiv.org/abs/2404.02258" target="_blank"> Mixture-of-Depths: Dynamically allocating compute in transformer-based language models </a></U>


Let's dive in!



## **Understanding the problem: Uniform computation in Transformers**
These days, most language models are based on Transformers, and we stack these blocks to make big models. When given an input sequence, tokens pass through these blocks to predict the next token. The problem is that the models spread computations uniformly across input sequences. Transformers use the same amount of computation for essential tokens as for non-essential ones. For instance, predicting a token within a sentence is cheaper than predicting the first token of the next sentence. Researchers want to address this issue by making Transformers focus on important tokens by allocating less computing resources.

## **Conditional computation for Transformers**
- Early exiting
  <p align="center">
    <img src=./Early_Exiting.png> 
</p>
  Instead of passing through all layers, the model can stop early if it is confident enough about its prediction. This saves computation time and resources. Large pre-trained models like BERT can use early exiting ot maintain performance while reducing computational load.
  
- CoLT5

- Mixture of Experts (MoE)

  MoE is an model which consists of parallel expert models which is fitted to certain domains. Like MoD, token-level routing decisions are made across the network depth. Difference between MoD is, MoD chooses path to transformer or to residual connection, MoE chooses path to transformer(Expert) or to transformer(Expert) or both.
  
## **Overview to Mixture-of-Depths (MoD)**
Our goal is to reduce the overall FLOPs by focusing on essential tokens and relatively fewer on non-essential tokens. The router is responsible for determining the path each token should take. A trained router evaluates whether a token is necessary. If the token is deemed essential, it passes through self-attention and the subsequent MLP (requiring FLOPs). Otherwise, it bypasses these stages via a residual connection (saving FLOPs).
<p align="center">
    <img src=./Mixture-of-Depths.png> 
</p>
 Above image depicts the path of a MoD (Model of Decoding) with an input sequence length of 64. The purple color shows the computation performed by that layer and the orange color shows the path taken by the residual connection.

(삭제예정)
MoE is an model which consists of parallel expert models which is fitted to certain domains.
Like MoD, token-level routing decisions are made across the network depth.
Difference between MoD is, MoD chooses path to transformer or to residual connection, MoE chooses path to transformer(Expert) or to transformer(Expert) or both.

## **Routing schemes**
Routing implementation is the most crucial part of MoD. The authors compare three routing schemes, demonstrating that MoD is an efficient approach.

<p align="center">
    <img src=./Routing_Schemes.png> 
</p>

### **Token-choice routing**
Token-choice routing is a method where each tokens select the path it will follow. The router produces probability distributions for each token across the computational paths. Based on this distribution, each token chooses its preferred path at each layer.
  
In token-choice routing, tokens have the flexibility to select their path, allowing for dynamic processing. However, this can lead to path balancing issues as all tokens might preger on the same path. It causes potential overloads on specific paths. To mitigate it, auxility loss is used to ensure that most tokens do not prefer on a single path.
  
### **Expert-choice routing**
Expert-choice routing is the reverse of token-choice routing. Similar to token-choice routing, the router produces a probability distribution for each token. In expert-choice routing, instead of tokens selecting their paths, each path selects the top-{{< katex >}}k{{< /katex >}} tokwns based on the tokens' preferences.

Using this method ensures that each paths receives k tokens, maintauing balance among the paths. However, some tokens may not be selected beacuse there might be common tokens that multiple paths prefer.

### **Expert-choice MoD**
This method applies expert-choice routing but uses only a single expert. Since only a single path is utilized, if {{< katex >}}k{{< /katex >}} is less than the sequence length, not all tokens need to undergo self-attention and MLP computation.

For the following reasons, the authors decided to use Expert-choice routing and utilize only single paths:
- Efficiency of computation
  
  Don't need for an auxiliary balancing loss
- Simplicity of implementation
  
  Simply can choose the tokens with the highest weight in order
- Clear criteria
  
  Can guarantee that the most important token is calculated since the top-{{< katex >}}k{{< /katex >}} tokens are independent on magnitude of router weights

  Top-{{< katex >}}k{{< /katex >}} can divide clearly tokens into two mutually sets

## **Implementation**
MoD Transformers는 다음과 같은 방식으로 작동한다.
1. Calculate routing weights
   {{< katex display=true >}}
x^{l+1}_i=\begin{cases}r^{l}_i f_i(\tilde{X}^l)+x^{l}_i, &    \text{if } r^{l}_i >  P_\beta(R^l)\\x^{l}_i, & \text{if }r^{l}_i <  P_\beta(R^l)\end{cases}
{{< /katex >}}
3. ssss
4. 
5. backward path 및 오차보정


## **More details**
### **Capacity**
### **Autoregressively sampling**
We're looking to implement expert-choice routing, but there is one distinct problem: top-k operations rely on future tokens! Our goal is for each token to determine if it belongs to the top-k using routers. To do this, every token needs access to the router weights of future tokens. Unfortunately, we lack the ability to predict the future router weights and cannot employ autoregressive sampling. To solve this problem, the authors propose two methods.

- Simple auxiliary loss
  <p align="center">
    <img src=./Routing_Analysis.png> 
  </p>
  Designing an additional binary cross-entropy loss function at the router's output can resolve this issue. By incorporating this, the value of tokens in the top-k is guided to be greater than 0.5, while the value of tokens are not in the top-k is guided to be less than 0.5. As token passes through the router, they are categorized into top-k set if their value exceeds 0.5. Then it passes through the self-attention and subsequent MLP. Conversely, tokens with values below 0.5 passs through the residual connection. Integrating such a function impacts the primary language modeling objective approximately 0.2-0.3%. We believe this likely refers to the extent to which performance and inference time are affected.
    
- Small auxiliary MLP predictor
  
  The second method does not affect the primary language modeling objective at all. The authors design a new MLP layer that functions as a binary classifier to determine wheather a token is in top-k during the training process. This classifer is trained to make these demterminations, and it is used in real-time during the autoregressive sampling process.

With these methods, authors could sample autoregressively by choosing to route tokens to or around a block based on the router's outer which is not depends on the future tokens. They provide empirical result that auxiliary task achieved 99% accuracy.

## **Open source MoD** (not official)
The followuing is an implementation of MoD that supports various LM such as Mixtral, LLama3 and BLOOM. It implements MoD using PyTorch and Hugging Face Transformers library.

LINK: https://github.com/astramind-ai/Mixture-of-depths

<details>
<summary>Code Details</summary>

The code operates in the following steps:
1. Token Weight Calculation
   
   '''class TokenRouter(nn.Module):
    def __init__(self, embed_dim):
        super().__init__()
        self.weight_predictor = nn.Linear(embed_dim, 1)

    def forward(self, x):
        weights = self.weight_predictor(x).squeeze(-1)  # [batch_size, seq_len]
        return weights
   '''
   
   The **TokenRouter** module caculates weights for each token based on its embedding. This is done using a lnear layer appleid to the embeddingsm resulting in a weight value for each token.
3. Selective Processing
   
   The processing occurs in the **MoD** module's forward pass
   - First token weights are calculated using **TokenRouter**
   - By a capacity paratmter, the number of tokens are determined. They undergo self-attention and MLP computation.
5. Application to Hugging Face Models
   
   **apply_mod_to_hf** function applies the MoD mechanism to an existing Hugging Face model.

</details>

## **Results**
<p align="center">
    <img src=./result2.png>
</p>
The figure above shows the results of training all models with the same number of FLOPs(6e18), regardless of the parameter size. The compared models are the Baseline (isoFLOP optimal baseline, vanilla transformer) and models with MoD applied, set to have either 12.5% capacity or 50% capacity. In the case of random routing, it does not follow the top-k metric but simply randomly chooses whether a token will go to the residual path or the attention + MLP layer path. Additionally, in the case of every 2, it means that the MoD method is not applied to all layers, but only to one out of every two layers. Therefore, from the top-left graph, the 12.5% capacity MoE loss value is less than the baseline model's. The two top-middle graphs show the actual training loss graphs for the points plotted in the left graph, where MoD with 12.5% capacity generally results in lower loss values than baseline. In the case of the right graph, the plotted points #1 & #3 and  #2 & #4 pairs are models of the same parameter size, with MoD applied, it not only has a loss value lower, but it also has an approximately 66% faster performance than original one.

<p align="center">
    <img src=./result1.png>
</p>
In this figure, the Training FLOPs budget is limited to 6e18, 2e19, and 1e20, compared with the isoFLOP baseline and 12.5% capacity MoD. At a glance, the  top-left graph shows that the isoFLOP baseline has a slightly better loss when the number of parameters is small(There’s a crossing point!) However, when the x-axis is converted from Parameters to FLOPs per FFW (Forward Pass) as shown in the top-right graph, it confirms that MoD is better than the baseline in all cases.

<p align="center">
    <img src=./result3.png>
</p>
In this figure, the top-left graph shows the performance degradation in auto-regressive evaluation using predictions using the MLP layer not the top-k routing mechanism, which is non-causal and can’t be used in this case. The author argues that the reason for this performance drop in auto-regressive case is that the prediction performance through the MLP layer is only about 97%, as shown in the top-right graph, but they stress that only minimal degradation occurs.

<p align="center">
    <img src=./result4.png>
</p>
This figure shows the performance of MoDE and its two proposed structures. The top-left graph demonstrates that the performance of MoDE is better than both the Baseline and MoE. The right side explains the structures of Staged MoDE and Integrated MoDE. In Staged MoDE, two routers are deployed to first for determine the depth(MoD) and second for the expert(MoE). In Integrated MoDE, as the name implies, the MoD Router and MoE Router are integrated to one single Router that can simultaneously decide whether to select an expert or the residual path (depth). The paper mentions that the former is computationally efficient as it can skip self-attention operations through the MoD router, and the latter has better performance as the router mechanism is unified and self-attention operations are always performed.

## **Conclusion and discussion**
결론 + 내 생각

## **Some resources**
참고문헌 정리
