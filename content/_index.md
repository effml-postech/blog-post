---
weight: 1
bookFlatSection: true
title: "Spring 2024"
---

# Blog-Post-Assignment (Spring 24)

This is the Github page for the class, Efficient ML Systems (EECE695D-01).  Students (will) upload a blog post reviewing the paper.

## Guideline for Students
* You toggle the menu and find your paper number. Then, into the page, you can easily find the ```Edit this page``` button.
     * You edit the markdown file. If you want to attach the image (e.g. pipeline), <br> you easily add file using ```Upload files``` button:
          {{< figure src="./example.png" alt="." width="1000" height="2000" >}}

     ðŸ’¡ You can reference TA's example post ðŸ‘‰ [LINK](https://effml-postech.github.io/docs/spring24/00_taco_example/)
     * Posted how to use the KaTex

## Contact
If you have any questions, please feel free to contact TA (hagyeonglee@postech.ac.kr).

## Paper List

| Paper No. | Title | Team |
|:---:|:---:|:---:|
| 0 | **_(example)_** [Neural Image Compression with Text-guided Encoding for both Pixel-level and Perceptual Fidelity](https://arxiv.org/abs/2403.02944) | Hagyeong Lee |
| 1 | [Is Bigger Edit Batch Size Always Better? -- An Empirical Study on Model Editing with Llama-3         	](https://arxiv.org/abs/2405.00666) | Jin Hyun,  Gyuhyun Jung |
| 2 | [Spectrally Pruned Gaussian Fields with Neural Compensation                          	](https://arxiv.org/pdf/2405.00676) | Donggeon Lee,  Chiho yoon |
| 3 | [Unit Scaling: Out-of-the-Box Low-Precision Training                              	](https://arxiv.org/abs/2303.11257) | SeongRok Moon,  Changyoung Ju |
| 4 | [Better & Faster Large Language Models via Multi-token Prediction                       	](https://arxiv.org/abs/2404.19737) | Jinoh Cho,  Seonghyeon Park |
| 5 | [Lossless Self-Speculative Decoding via Double Early Exiting                          	](https://arxiv.org/abs/2404.18911) | Nayoung Kwon, Jiwoong Im |
| 6 | [XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference                    	](https://arxiv.org/abs/2404.15420) | Hyundong Kim, Sangil Han |
| 7 | [VeRA: Vector-based Random Matrix Adaptation                                  	](https://arxiv.org/abs/2310.11454) | Kyumin Cho, Sejin Park |
| 8 | [Mixture of LoRA Experts                                            	](https://arxiv.org/abs/2404.13628) | Jegwang Ryu, Sangbeom Ha |
| 9 | [MobileNetV4 -- Universal Models for the Mobile Ecosystem                           	](https://arxiv.org/abs/2404.10518) | JoonSeok Kim, DongGyu Kim |
| 10 | [Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length               	](https://arxiv.org/pdf/2404.08801) | Hyunho Kook |
| 11 | [Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention            	](https://arxiv.org/abs/2404.07143) | Younghyun Cho, Sangjun Lee |
| 12 | [Scaling (Down) CLIP: A Comprehensive Analysis of Data, Architecture, and Training Strategies         	](https://arxiv.org/abs/2404.08197) | Junkyeong Park, Harit Keawmuang |
| 13 | [A Large-Scale Exploration of Î¼-Transfer                                    	](https://arxiv.org/abs/2404.05728) | Jeonghyun Choi, Minhye Choo |
| 14 | [BinaryDM: Towards Accurate Binarization of Diffusion Model                          	](https://arxiv.org/abs/2404.05662) | Junhyuk So, Juncheol Shin |
| 15 | [Training LLMs over Neurally Compressed Text                                  	](https://arxiv.org/abs/2404.03626) | Seonghyun Park, Jiwoo Kim |
| 16 | [Mixture-of-Depths: Dynamically allocating compute in transformer-based language models            	](https://arxiv.org/abs/2404.02258) | Minjae Park, Inkwan Hwang |
| 17 | [QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs                             	](https://arxiv.org/abs/2404.00456) | MyeongJi Yun, Jung Gyu Min |
| 18 | [ViTAR: Vision Transformer with Any Resolution                                 	](https://arxiv.org/abs/2403.18361) | Jungwon Lee, Minsang Seok |
| 19 | [LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning           	](https://arxiv.org/abs/2403.17919) | Sungbin Shin, Dongyeop Lee |
| 20 | [Evolutionary Optimization of Model Merging Recipes                              	](https://arxiv.org/abs/2403.13187) | Youngkil Song, Jaehyeon Park |
| 21 | [A Unified Framework for Model Editing                                     	](https://arxiv.org/abs/2403.14236) | Jonghyun Chae, Donggeun An |
| 22 | [Larimar: Large Language Models with Episodic Memory Control                          	](https://arxiv.org/abs/2403.11901) | Sunggyu Jang, Hyeonwoo Park |
| 23 | [Beyond Language Models: Byte Models are Digital World Simulators                       	](https://arxiv.org/abs/2402.19155) | Dohun Kim, Yeongwoo Kim |
| 24 | [LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression          	](https://arxiv.org/abs/2403.12968) | Seungjoo Shin, Sua Choi |
| 25 | [Merging Text Transformer Models from Different Initializations                        	](https://arxiv.org/abs/2403.00986) | Minwoo Kim, Kyungtae Kim |